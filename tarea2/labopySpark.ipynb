{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXu9uUgx7R87"
      },
      "source": [
        "![CMCC](https://scontent.fcuz1-1.fna.fbcdn.net/v/t1.6435-9/169999414_134997141968151_5389622841264143457_n.jpg?_nc_cat=100&ccb=1-5&_nc_sid=09cbfe&_nc_eui2=AeGYHrPMueJlavl7eDql2QOI2St2L7MLFHTZK3YvswsUdPzk3Ovp__UvRhUQlcFyH2o&_nc_ohc=KZJCq4jFuhMAX9h5HUI&_nc_ht=scontent.fcuz1-1.fna&oh=00_AT_sGFdClC9xMtR4S8p-ilZYOR4ycFv1uCTqoKj-frAi8g&oe=61F5FE4C)\n",
        "# **Spark + Python = PySpark**\n",
        "\n",
        "#### Este notebook presenta los conceptos básicos de Spark a través de su interfaz de lenguaje Python. Como aplicación inicial usaremos el ejemplo clásico del contador de palabras. Con este ejemplo, puede comprender la lógica de programación funcional para las diversas tareas de exploración de datos distribuidos.\n",
        "\n",
        "#### Para esto usaremos el libro de texto [Obras completas de William Shakespeare] (http://www.gutenberg.org/ebooks/100) obtenido del [Proyecto Gutenberg] (http://www.gutenberg.org/ wiki / Página_principal). Veremos que este mismo algoritmo se puede utilizar para textos de cualquier tamaño.\n",
        "\n",
        "#### **Este notebook contiene:**\n",
        "#### *Parte 1:* Creación de una base RDD y RDD de tupla\n",
        "#### *Parte 2:* Manejo de RDD de tuplas\n",
        "#### *Parte 3:* Encontrar palabras sueltas y calcular promedios\n",
        "#### *Parte 4:* Aplicar el recuento de palabras a un archivo\n",
        "#### *Parte 5:* Similitud entre objetos\n",
        "#### Para los ejercicios es recomendable consultar la documentación de [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##instalacion de pyspark colab\n"
      ],
      "metadata": {
        "id": "oD83eClG8Arz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls\n",
        "!ls /usr/lib/jvm/\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!ls /usr/lib/jvm/\n",
        "#descargar apache pyspark (CON LA ULTIMA VERSION DISPONIBLE)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykgzou6x7-3-",
        "outputId": "d841929d-e646-4598-d62a-b7a9307249e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "sample_data\n",
            "default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,489 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [716 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [749 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,929 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Fetched 13.7 MB in 5s (3,044 kB/s)\n",
            "Reading package lists... Done\n",
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n",
            "spark-3.2.0-bin-hadoop3.2/\n",
            "spark-3.2.0-bin-hadoop3.2/NOTICE\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/python_executable_check.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/autoscale.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/decommissioning.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.2.0-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.2.0-bin-hadoop3.2/jars/\n",
            "spark-3.2.0-bin-hadoop3.2/jars/RoaringBitmap-0.9.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-graphx_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-metrics-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/okhttp-3.12.12.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-shims-common-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spire_2.12-0.17.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-serde-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/zookeeper-3.6.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-jdbc-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/metrics-jvm-4.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/httpcore-4.4.14.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-codec-1.15.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-repl_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/zookeeper-jute-3.6.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-sketch_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-policy-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jline-2.14.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.12.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-coordination-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/metrics-json-4.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/xbean-asm9-shaded-4.20.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/cats-kernel_2.12-2.1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jta-1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-streaming_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-extensions-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-client-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-common-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-discovery-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/objenesis-2.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jpam-1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/lapack-2.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-network-common_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-common-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-core-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/snakeyaml-1.27.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.3.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/avro-1.10.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/antlr4-runtime-4.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/avro-mapred-1.10.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-node-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/paranamer-2.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-core_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/json4s-core_2.12-3.7.0-M11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-catalyst_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/stream-2.9.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-flowcontrol-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-shims-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/gson-2.2.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-container-servlet-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-compress-1.21.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/metrics-graphite-4.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-common-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/opencsv-2.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-exec-2.3.9-core.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-rbac-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/derby-10.14.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hadoop-client-api-3.3.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-annotations-2.12.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-reflect-2.12.15.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/okio-1.14.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-certificates-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-shims-0.23-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hadoop-client-runtime-3.3.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-lang3-3.12.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/automaton-1.11-8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-lang-2.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-jackson-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/velocity-1.5.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/httpclient-4.5.13.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/JTransforms-3.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/json4s-scalap_2.12-3.7.0-M11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/zstd-jni-1.5.0-4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-server-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/aircompressor-0.21.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/joda-time-2.10.10.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-batch-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-mesos_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/guava-14.0.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-mllib_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-autoscaling-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-networking-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-launcher_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-events-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-hadoop-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/oro-2.0.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/metrics-jmx-4.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arrow-memory-netty-2.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/core-1.1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/orc-core-1.6.11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-compiler-2.12.15.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/json-1.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-library-2.12.15.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arrow-vector-2.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/lz4-java-1.7.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/snappy-java-1.1.8.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/activation-1.1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/annotations-17.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/generex-1.0.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.12.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/orc-shims-1.6.11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/rocksdbjni-6.20.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-scheduling-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-admissionregistration-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-datatype-jsr310-2.11.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/shims-0.9.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-cli-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-storageclass-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-beeline-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-sql_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-apiextensions-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/ST4-4.0.4.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/chill_2.12-0.10.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/json4s-jackson_2.12-3.7.0-M11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-yarn_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/breeze-macros_2.12-1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-tags_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/xz-1.8.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-databind-2.12.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/blas-2.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/minlog-1.3.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-hk2-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-container-servlet-core-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-tags_2.12-3.2.0-tests.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-common-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spire-util_2.12-0.17.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-hive_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jersey-client-2.34.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-net-3.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/janino-3.0.16.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/javolution-5.5.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/transaction-api-1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-io-2.8.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jackson-core-2.12.3.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-cli-1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-llap-common-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/tink-1.6.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/algebra_2.12-2.0.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hadoop-shaded-guava-1.1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/metrics-core-4.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/orc-mapreduce-1.6.11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-format-structures-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arrow-format-2.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/json4s-ast_2.12-3.7.0-M11.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-column-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arrow-memory-core-2.0.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/arpack-2.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/commons-text-1.6.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/parquet-encoding-1.12.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/kubernetes-model-apps-5.4.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/breeze_2.12-1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/pyrolite-4.30.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/chill-java-0.10.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/spark-kvstore_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/log4j-1.2.17.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/netty-all-4.1.68.Final.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/avro-ipc-1.10.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/hive-metastore-2.3.9.jar\n",
            "spark-3.2.0-bin-hadoop3.2/jars/py4j-0.10.9.2.jar\n",
            "spark-3.2.0-bin-hadoop3.2/data/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/als/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/als/test.data\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/pic_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/license.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/images/license.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/ridge-data/\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/kmeans_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/pagerank_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/mllib/gmm_data.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/graphx/\n",
            "spark-3.2.0-bin-hadoop3.2/data/graphx/users.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/graphx/followers.txt\n",
            "spark-3.2.0-bin-hadoop3.2/data/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/data/streaming/AFINN-111.txt\n",
            "spark-3.2.0-bin-hadoop3.2/R/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/sparkr.zip\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/profile/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/INDEX\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/R/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/doc/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/doc/index.html\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/html/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/html/R.css\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/worker/\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n",
            "spark-3.2.0-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.2.0-bin-hadoop3.2/README.md\n",
            "spark-3.2.0-bin-hadoop3.2/RELEASE\n",
            "spark-3.2.0-bin-hadoop3.2/yarn/\n",
            "spark-3.2.0-bin-hadoop3.2/yarn/spark-3.2.0-yarn-shuffle.jar\n",
            "spark-3.2.0-bin-hadoop3.2/LICENSE\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-workers.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-master.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/workers.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-worker.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/spark-config.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-history-server.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-slaves.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/spark-daemon.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-worker.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/decommission-worker.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/decommission-slave.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/slaves.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-history-server.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-thriftserver.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-thriftserver.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-slave.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-all.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-slave.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/spark-daemons.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-workers.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-slaves.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-all.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.2.0-bin-hadoop3.2/sbin/stop-master.sh\n",
            "spark-3.2.0-bin-hadoop3.2/examples/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/als.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/dataframe.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/extensions/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/people.json\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/users.avro\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/people.csv\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/users.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/META-INF/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/META-INF/services/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/users.orc\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/dir1/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/user.avsc\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/people.txt\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/resources/employees.json\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scripts/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/kmeans.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/als.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/pagerank.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sort.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/pi.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_sessionization.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.2.0-bin-hadoop3.2/examples/jars/\n",
            "spark-3.2.0-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.0.jar\n",
            "spark-3.2.0-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.2.0-bin-hadoop3.2/conf/\n",
            "spark-3.2.0-bin-hadoop3.2/conf/metrics.properties.template\n",
            "spark-3.2.0-bin-hadoop3.2/conf/workers.template\n",
            "spark-3.2.0-bin-hadoop3.2/conf/fairscheduler.xml.template\n",
            "spark-3.2.0-bin-hadoop3.2/conf/log4j.properties.template\n",
            "spark-3.2.0-bin-hadoop3.2/conf/spark-defaults.conf.template\n",
            "spark-3.2.0-bin-hadoop3.2/conf/spark-env.sh.template\n",
            "spark-3.2.0-bin-hadoop3.2/bin/\n",
            "spark-3.2.0-bin-hadoop3.2/bin/sparkR.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/sparkR\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-submit\n",
            "spark-3.2.0-bin-hadoop3.2/bin/pyspark2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-class\n",
            "spark-3.2.0-bin-hadoop3.2/bin/pyspark.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-submit2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/load-spark-env.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-sql\n",
            "spark-3.2.0-bin-hadoop3.2/bin/docker-image-tool.sh\n",
            "spark-3.2.0-bin-hadoop3.2/bin/find-spark-home.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/load-spark-env.sh\n",
            "spark-3.2.0-bin-hadoop3.2/bin/pyspark\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-shell.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-shell2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-submit.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/beeline.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/find-spark-home\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-class.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/sparkR2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/beeline\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-class2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-sql.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/run-example\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-shell\n",
            "spark-3.2.0-bin-hadoop3.2/bin/run-example.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/bin/spark-sql2.cmd\n",
            "spark-3.2.0-bin-hadoop3.2/python/\n",
            "spark-3.2.0-bin-hadoop3.2/python/.gitignore\n",
            "spark-3.2.0-bin-hadoop3.2/python/run-tests-with-coverage\n",
            "spark-3.2.0-bin-hadoop3.2/python/mypy.ini\n",
            "spark-3.2.0-bin-hadoop3.2/python/pylintrc\n",
            "spark-3.2.0-bin-hadoop3.2/python/MANIFEST.in\n",
            "spark-3.2.0-bin-hadoop3.2/python/README.md\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_coverage/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_coverage/conf/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/run-tests.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/setup.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/userlibrary.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/hello/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/hello/sub_hello/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/hello/hello.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/people.json\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/people_array.json\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/text-test.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/ages.csv\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/test_support/sql/people1.json\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_join.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_series.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_dataframe.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/indexes/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/indexes/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/indexes/test_category.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/indexes/test_datetime.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/indexes/test_base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_window.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_indexops_spark.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_stats.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_extension.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_default_index.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_expanding.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_series_datetime.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_indexing.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_typedef.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_numpy_compat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_repr.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_namespace.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_series_conversion.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_internal.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_series_string.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_groupby.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_csv.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_dataframe_conversion.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/data_type_ops/test_base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_frame_spark.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_config.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_categorical.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_frame_plot.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/plot/test_series_plot.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_rolling.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_reshape.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_sql.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/tests/test_spark_functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/sql_processor.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/numpy_compat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/usage_logging/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/usage_logging/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/usage_logging/usage_logger.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/spark/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/spark/functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/spark/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/spark/accessors.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/spark/utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/frame.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/window.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/numeric.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/datetimes.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/category.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexes/multi.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/namespace.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/mlflow.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/indexing.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/strings.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/extensions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/accessors.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/config.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/groupby.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/internal.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/frame.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/window.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/groupby.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/series.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/indexes.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/missing/common.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/ml.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/series.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/_typing.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/datetimes.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/complex_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/binary_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/categorical_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/string_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/date_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/udt_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/null_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/datetime_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/num_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/data_type_ops/boolean_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/plot/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/plot/core.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/plot/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/plot/plotly.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/plot/matplotlib.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/typedef/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/typedef/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/typedef/typehints.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/typedef/string_typehints.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/categorical.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/generic.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/pandas/exceptions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/_typing.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/pandasutils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/accumulators.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/rddsampler.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/install.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/status.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/evaluation.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/_typing.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/recommendation.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tuning.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/fpm.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/base.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/feature.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/wrapper.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/stat.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/stat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/image.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/classification.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/common.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/pipeline.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/clustering.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/regression.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/param/shared.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/feature.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/classification.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tree.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/util.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tuning.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/fpm.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/regression.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/functions.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/base.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/image.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/tree.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/clustering.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/common.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/linalg/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/context.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/find_spark_home.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/serializers.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/java_gateway.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/traceback_utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/profile.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/information.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/requests.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/information.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/requests.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resource/profile.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/conf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resultiterable.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/version.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/__pycache__/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/files.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/evaluation.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/fpm.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/feature.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/classification.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/common.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/random.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/clustering.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/regression.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/feature.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/classification.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tree.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/util.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/regression.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/random.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/test.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/tree.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/common.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/linalg/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/mllib/util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/resultiterable.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/profiler.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/statcounter.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/join.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/daemon.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/storagelevel.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/version.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/py.typed\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/files.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/worker.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/util.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/statcounter.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/conf.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/shell.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/dstream.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/kinesis.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/context.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/listener.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/listener.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/streaming/util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/accumulators.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/profiler.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/status.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/broadcast.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/types.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/_typing.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/streaming.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/context.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/column.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/window.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/udf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/conf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/column.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/group.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/catalog.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/group.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/context.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/dataframe.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/types.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/functions.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/conf.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/udf.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/avro/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/avro/functions.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/readwriter.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/window.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/session.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/streaming.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/python/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/python/pyspark/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/shuffle.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/taskcontext.pyi\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/taskcontext.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/_globals.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/broadcast.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/util.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark/storagelevel.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/.coveragerc\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/make2.bat\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.ss.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/window.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/extensions.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/ml.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/frame.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/io.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/series.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/indexing.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/general_functions.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.pandas/groupby.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.sql.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/getting_started/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/getting_started/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/getting_started/quickstart_ps.ipynb\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/getting_started/quickstart_df.ipynb\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/getting_started/install.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/conf.py\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_templates/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_templates/autosummary/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_static/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_static/copybutton.js\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_static/css/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/setting_ide.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/debugging.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/testing.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/development/contributing.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/types.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/options.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/pandas_on_spark/faq.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/sql/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/sql/arrow_pandas.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/user_guide/sql/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/index.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/koalas_to_pyspark.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/make.bat\n",
            "spark-3.2.0-bin-hadoop3.2/python/docs/Makefile\n",
            "spark-3.2.0-bin-hadoop3.2/python/lib/\n",
            "spark-3.2.0-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip\n",
            "spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip\n",
            "spark-3.2.0-bin-hadoop3.2/python/run-tests\n",
            "spark-3.2.0-bin-hadoop3.2/python/setup.cfg\n",
            "spark-3.2.0-bin-hadoop3.2/python/dist/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/requires.txt\n",
            "spark-3.2.0-bin-hadoop3.2/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-respond.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-janino.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-spire.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-join.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-javassist.html\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-scala.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-jline.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-blas.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n",
            "spark-3.2.0-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 66.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=a944589e0bc6d8578afb750dde50c73291e81d9d93e0a3bc4038dfa712679f03\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eowtVWOr7R9D"
      },
      "source": [
        "### **Parte 1: Creación y manipulación de RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBFGKyW7R9F"
      },
      "source": [
        "#### En esta parte del notebook crearemos una base RDD a partir de una lista con el comando `parallelize`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cOob7X97R9G"
      },
      "source": [
        "#### **(1a) Creando una base RDD**\n",
        "#### Podemos crear una base de datos RDD de diferentes tipos y fuente Python con el comando `sc.parallelize (fuente, particiones)`, siendo fuente una variable que contiene los datos (ej .: una lista) y particiona el número de particiones trabajar en paralelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqDqBaId7XHT",
        "outputId": "892ec3b1-31de-4fee-b474-34917013c6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 68.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=e6ba47e8ed892642b6e381c4c6d8229374bac2cfee03d7495e404e110b23ef75\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.2\n",
            "    Uninstalling py4j-0.10.9.2:\n",
            "      Successfully uninstalled py4j-0.10.9.2\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.0\n",
            "    Uninstalling pyspark-3.2.0:\n",
            "      Successfully uninstalled pyspark-3.2.0\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QxHgRfat7R9I"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "import pyspark  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peg23yAZ7R9K",
        "outputId": "ded5ad23-9ad9-4584-aae8-f347bcd10988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "ListaPalavras = ['gato', 'elefante', 'rato', 'rato', 'gato']\n",
        "palavrasRDD = sc.parallelize(ListaPalavras, 4)\n",
        "print(type(palavrasRDD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz7UKk967R9N"
      },
      "source": [
        "#### **(1b) Plural**\n",
        "\n",
        "#### Vamos a crear una función que transforme una palabra en plural agregando una letra 's' al final de la cadena. A continuación, usemos la función `map()` para aplicar la transformación a cada palabra del RDD.\n",
        "\n",
        "#### En Python (y muchos otros lenguajes) la concatenación de cadenas es costosa. Una mejor alternativa es crear una nueva cadena usando [`str.format()`] (https://docs.python.org/2/library/string.html#format-string-syntax).\n",
        "\n",
        "#### Nota: La cadena entre los conjuntos de tres comillas representa la documentación de la función. Esta documentación se muestra con el comando `help()`. Usaremos la estandarización de documentación sugerida para Python, mantendremos esta documentación en inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kq-Pqs-7R9Q",
        "outputId": "5a284558-9e14-4afc-f38b-b969298f8b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gatos\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "def Plural(palavra):\n",
        "    \"\"\"Adds an 's' to `palavra`.\n",
        "\n",
        "    Args:\n",
        "        palavra (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: A string with 's' added to it.\n",
        "    \"\"\"\n",
        "    return f\"{palavra}s\"\n",
        "print(Plural('gato'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCJ0t--e7R9S",
        "outputId": "8c32e3ff-fcb6-408a-bad9-094ee34c9357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function Plural in module __main__:\n",
            "\n",
            "Plural(palavra)\n",
            "    Adds an 's' to `palavra`.\n",
            "    \n",
            "    Args:\n",
            "        palavra (str): A string.\n",
            "    \n",
            "    Returns:\n",
            "        str: A string with 's' added to it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(Plural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVtbAQz7R9U",
        "outputId": "b82d7428-5a9d-449b-e74b-754966855930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert Plural('rato')=='ratos', 'resultado incorreto!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V8kKH-37R9V"
      },
      "source": [
        "#### **(1c) Aplicar la función a RDD**\n",
        "#### Pluralice cada palabra de nuestro RDD usando [map()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)\n",
        "\n",
        "#### A continuación, usaremos el comando [collect()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) que devuelve el RDD como una lista de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnyNJoa_7R9X",
        "outputId": "102894a4-aeed-4eb2-e816-fc867dd9dc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "pluralRDD = palavrasRDD.map(Plural)\n",
        "print (pluralRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRGX1vhw7R9Y",
        "outputId": "99beb44a-d01e-4a99-be75-c828b3dd2117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzo6HPPV7R9Z"
      },
      "source": [
        "#### **Nota:** use el comando `collect()` solo cuando esté seguro de que la lista cabe en la memoria. Para guardar los resultados en un archivo de texto o base de datos, usaremos otro comando."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rgdUKD7R9a"
      },
      "source": [
        "#### **(1d) Usando una función `lambda`**\n",
        "#### Repite la creación de un RDD plural, pero usando una función lambda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zgdo_jj7R9b",
        "outputId": "058504f6-06ca-4529-914f-0a2d797649ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "pluralLambdaRDD = palavrasRDD.map(lambda x: f\"{x}s\")\n",
        "print (pluralLambdaRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXn7a3ln7R9c",
        "outputId": "a0b99c26-78dd-4b2f-ea55-21e79fcc0545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralLambdaRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
        "print ('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32PH7y0M7R9d"
      },
      "source": [
        "#### **(1e) Tamaño de cada palabra**\n",
        "#### Ahora usa `map()` y una función `lambda` para devolver el número de caracteres en cada palabra. Utilice `collect()` para almacenar el resultado como listas en la variable de destino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQtIc37K7R9d",
        "outputId": "3b6c29e3-7b1f-43b1-a3c0-aff961584876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 9, 5, 5, 5]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "pluralTamanho = (pluralRDD\n",
        "                 .map(lambda x:len(x))\n",
        "                 .collect()\n",
        "                 )\n",
        "print (pluralTamanho)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5M2kCfI7R9e",
        "outputId": "6dfd157c-4526-42b5-b7dd-e56e8e09b977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert pluralTamanho==[5,9,5,5,5], 'valores incorretos'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKMMmvOR7R9f"
      },
      "source": [
        "#### **(1f) RDD de pares y tuplas**\n",
        "\n",
        "#### Para contar la frecuencia de cada palabra de forma distribuida, primero debemos asignar un valor a cada palabra en el RDD. Esto generará una base de datos (clave, valor). De esta forma podemos agrupar la base mediante la clave, calculando la suma de los valores asignados. En nuestro caso, asignemos el valor `1` a cada palabra.\n",
        "\n",
        "#### Un RDD que contiene la estructura de tupla clave-valor `(k, v)` se denomina RDD de tupla o * RDD de par *.\n",
        "\n",
        "#### Vamos a crear nuestro RDD a partir de pares usando la transformación `map ()` con una función `lambda ()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSNfUswM7R9g",
        "outputId": "0b3c5ac3-bb02-486e-8658-e5bf2bab3fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('gato', 1), ('elefante', 1), ('rato', 1), ('rato', 1), ('gato', 1)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "palavraPar = palavrasRDD.map(lambda x: (x, 1))\n",
        "print (palavraPar.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKoNKS-17R9g",
        "outputId": "ec589233-0ce4-4577-d003-187ad4e2b95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert palavraPar.collect() == [('gato',1),('elefante',1),('rato',1),('rato',1),('gato',1)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkJ1piZz7R9p"
      },
      "source": [
        "### **Parte 2: Manipulación de Tuplas RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8vN7hP7R9q"
      },
      "source": [
        "#### Manipulemos nuestro RDD para contar las palabras en el texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5M3n6_U7R9q"
      },
      "source": [
        "#### **(2a) Función `groupByKey ()`**\n",
        "\n",
        "#### La función [groupByKey ()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) agrupa todos los valores de un RDD por clave (primer elemento de la tupla) agregando los valores en una lista.\n",
        "\n",
        "#### Este enfoque tiene una debilidad porque:\n",
        "   + #### La operación requiere que los datos seccionados se muevan de forma masiva para que permanezcan en la partición correcta.\n",
        "   + #### Las listas pueden llegar a ser muy largas. Imagínese contar todas las palabras en Wikipedia: términos comunes como \"a\", \"e\" formarán una lista enorme de valores que podrían no caber en la memoria del proceso esclavo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VBmneS7R9r",
        "outputId": "fa5be185-d0e2-4558-e8d3-7b83a140ea15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elefante: [1]\n",
            "rato: [1, 1]\n",
            "gato: [1, 1]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "palavrasGrupo = palavraPar.groupByKey()\n",
        "for chave, valor in palavrasGrupo.collect():\n",
        "    valores = list(valor)\n",
        "    print(f'{chave}: {valores}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ten9oHk57R9r",
        "outputId": "a3ec585b-5637-48e0-fc17-2365a23255a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(palavrasGrupo.mapValues(lambda x: list(x)).collect()) == [('elefante', [1]), ('gato',[1, 1]), ('rato',[1, 1])], 'Valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NYvyPM17R9s"
      },
      "source": [
        "#### **(2b) Cálculo de los recuentos**\n",
        "#### Después de `groupByKey()` nuestro RDD contiene elementos compuestos por la palabra, como clave, y un iterador que contiene todos los valores correspondientes a esa clave.\n",
        "#### Usando la transformación `map()` y la función `sum()`, construye un nuevo RDD que consta de tuplas (clave, suma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rncpIx0X7R9t",
        "outputId": "a167e2b3-0fe6-4950-a2ea-379a308b7e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "contagemGroup = palavrasGrupo.map(lambda x:(x[0],sum(x[1])))\n",
        "print (contagemGroup.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgimY8p67R9t",
        "outputId": "cf6bc823-0a61-430d-eea2-6534337612b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagemGroup.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6tP0p9u7R9u"
      },
      "source": [
        "#### **(2c) `reduceByKey`**\n",
        "#### Un comando más interesante para contar es [reduceByKey ()] (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) que crea un nuevo RDD de tuplas.\n",
        "\n",
        "#### Esta transformación aplica la transformación `reduce()` vista en la lección anterior a los valores de cada clave. De esta manera, la función de transformación puede aplicarse a cada partición local y luego enviarse para la redistribución de la partición, reduciendo la cantidad de datos que se mueven y no manteniendo grandes listas en la memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVuRVAL07R9v",
        "outputId": "0a782184-81ea-4c74-d97c-43a0aa07ab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "contagem = palavraPar.reduceByKey(lambda x,y:x+y)\n",
        "print( contagem.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APVc44LK7R9v",
        "outputId": "0f967c5b-2d4f-4f34-9150-2975b8955b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagem.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDWAhDJ7R9w"
      },
      "source": [
        "#### **(2d) Comandos de agrupación**\n",
        "\n",
        "#### La forma más común de realizar esta tarea, a partir de nuestras RDD RDDwords, es encadenar el mapa y los comandos reduceByKey en una línea de comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMws_DGd7R9w",
        "outputId": "b625c7c2-1a3b-4b16-df5f-5db1dcf4b4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "contagemFinal = (palavrasRDD\n",
        "                 .map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "                 .collect())\n",
        "print (contagemFinal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgMDKPy37R9x",
        "outputId": "10e93d06-f366-4dac-afdd-7202443b58b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contagemFinal)==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fajmnelA7R9x"
      },
      "source": [
        "### **Parte 3: Encontrar las palabras únicas y calcular el recuento promedio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx3h3YJU7R9y"
      },
      "source": [
        "#### **(3a) Palabras únicas**\n",
        "\n",
        "#### Calcula el número de palabras únicas en RDD. Utilice la misma canalización que antes pero reemplazando `.collect()` con otro método de API RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgxbHkEJ7R9y",
        "outputId": "5cff93b4-b387-4428-939e-0917317bcb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "palavrasUnicas = palavrasRDD.distinct().count()\n",
        "print (palavrasUnicas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrgHjPbV7R9z",
        "outputId": "5c28ef29-fdc6-431c-8805-78d978bcbd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert palavrasUnicas==3, 'valor incorreto!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQElU4pN7R9z"
      },
      "source": [
        "#### **(3b) Cálculo del recuento promedio de palabras**\n",
        "\n",
        "#### Encuentra la frecuencia promedio de palabras usando RDD `count`.\n",
        "\n",
        "#### Tenga en cuenta que la función de comando `reduce()` se aplica a cada tupla de RDD. Para realizar la suma de los recuentos, primero es necesario mapear el RDD a un RDD que contenga solo los valores de frecuencia (sin las claves)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7w8pNr07R90",
        "outputId": "86a7c276-4e3e-420c-ef64-93edaa9d5538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "1.67\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# add é equivalente a lambda x,y: x+y\n",
        "from operator import add\n",
        "total = (contagem\n",
        "         .map(lambda x: x[1])\n",
        "         .reduce(add)) \n",
        "media = total / float(palavrasUnicas)\n",
        "print (total)\n",
        "print (round(media, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PruM_GW7R90",
        "outputId": "28b3f819-e37a-42be-f828-2fc9696c4491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert round(media, 2)==1.67, 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q17IGTZu7R90"
      },
      "source": [
        "### **Parte 4: Aplicar nuestro algoritmo a un archivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcndmVnM7R91"
      },
      "source": [
        "#### ** (4a) Función `contaPalavras` **\n",
        "\n",
        "#### Para aplicar nuestro algoritmo genéricamente a múltiples RDD, primero creemos una función para aplicarla a cualquier fuente de datos. Esta función recibe la entrada de un RDD que contiene una lista de claves (palabras) y devuelve un RDD de tuplas con las claves y su recuento en ese RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REC6Y4oh7R91",
        "outputId": "2a9f8a27-a0c8-4cfd-ebb2-faff00710570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "def contaPalavras(chavesRDD):\n",
        "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
        "\n",
        "    Args:\n",
        "        chavesRDD (RDD of str): An RDD consisting of words.\n",
        "\n",
        "    Returns:\n",
        "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
        "    \"\"\"\n",
        "    return (chavesRDD\n",
        "            .map(lambda x: (x, 1))\n",
        "            .reduceByKey(lambda x, y: x + y))\n",
        "print (contaPalavras(palavrasRDD).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smSi3iyx7R92",
        "outputId": "1074bba8-4106-4620-9d5c-e0aac4b0d658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert sorted(contaPalavras(palavrasRDD).collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTNVRCpl7R92"
      },
      "source": [
        "#### ** (4b) Normalizando el texto **\n",
        "\n",
        "#### Cuando trabajamos con datos reales, normalmente necesitamos estandarizar los atributos de tal manera que se ignoren las diferencias sutiles debidas a errores de medición o diferencias de estandarización. Para el siguiente paso, estandarizaremos el texto para:\n",
        "   + #### Estandarizar las mayúsculas de las palabras (todas en mayúsculas o todas en minúsculas).\n",
        "   + #### Eliminar puntuación.\n",
        "   + #### Eliminar espacios al principio y al final de la palabra.\n",
        " \n",
        "#### Crea una función `removerPontuacao` que convierte todo el texto a minúsculas, elimina los signos de puntuación y los espacios en blanco al principio o al final de la palabra. Para esto, use la biblioteca [re] (https://docs.python.org/2/library/re.html) para eliminar todo el texto que no sea letra, número o espacio, enlazando con las funciones de cadena para eliminar los espacios en blanco y convertir a minúsculas (consulte [Cadenas] (https://docs.python.org/2/library/stdtypes.html?highlight=str.lower#string-methods))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC8Oq-v07R93",
        "outputId": "327c7ef6-e32a-457f-9ad4-7dcc281bdd8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ola quem esta ai\n",
            "sem espaco esublinhado\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "import re\n",
        "def removerPontuacao(texto):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        texto (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower()\n",
        "print (removerPontuacao('Ola, quem esta ai??!'))\n",
        "print (removerPontuacao(' Sem espaco e_sublinhado!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HiqKFkJ7R94",
        "outputId": "f59e154b-cd3c-49d9-88a9-fef0485bbc88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert removerPontuacao(' O uso de virgulas, embora permitido, nao deve contar. ')=='o uso de virgulas embora permitido nao deve contar', 'string incorreta!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYgz0ym7R94"
      },
      "source": [
        "#### **(4c) Cargando archivo de texto**\n",
        "\n",
        "#### Para la siguiente parte usaremos el libro [Obras completas de William Shakespeare] (http://www.gutenberg.org/ebooks/100) de [Proyecto Gutenberg] (http://www.gutenberg.org / wiki / Página_principal).\n",
        "\n",
        "#### Para convertir un texto en un RDD, usamos la función `textFile()` que toma como entrada el nombre del archivo de texto que queremos usar y el número de particiones.\n",
        "\n",
        "#### El nombre del archivo de texto puede hacer referencia a un archivo local o un URI de archivo distribuido (por ejemplo, hdfs: //).\n",
        "\n",
        "#### Apliquemos también la función `removeScore()` para normalizar el texto y verifiquemos las primeras 15 líneas con el comando `take()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slwk36ER9EUk"
      },
      "source": [
        "\n",
        "\n",
        "> cuando se trabaja en google colab lo unico seria subir el archivo pg100.txt entonces para abrir el archivo en codigo seria  solo direccionar `pg100.txt`\n",
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apenas execute a célula\n",
        "import os.path\n",
        "\n",
        "arquivo = os.path.join('pg100.txt') \n",
        "\n",
        "# lê o arquivo com textFile e aplica a função removerPontuacao        \n",
        "shakesRDD = (sc\n",
        "             .textFile(arquivo, 8)\n",
        "             .map(removerPontuacao)\n",
        "             )\n",
        "\n",
        "# zipWithIndex gera tuplas (conteudo, indice) onde indice é a posição do conteudo na lista sequencial\n",
        "# Ex.: sc.parallelize(['gato','cachorro','boi']).zipWithIndex() ==> [('gato',0), ('cachorro',1), ('boi',2)]\n",
        "# sep.join() junta as strings de uma lista através do separador sep. Ex.: ','.join(['a','b','c']) ==> 'a,b,c'\n",
        "print ('\\n'.join(shakesRDD\n",
        "                .zipWithIndex()\n",
        "                .map(lambda linha: '{0}: {1}'.format(linha[0],linha[1]))\n",
        "                .take(15)\n",
        "               ))"
      ],
      "metadata": {
        "id": "PSOhgdRj_gyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2FlGLuL7R95"
      },
      "source": [
        "#### **(4d) Extrayendo las palabras**\n",
        "#### Antes de que podamos usar nuestra función `contaPalavras()`, todavía tenemos que trabajar sobre nuestro RDD:\n",
        "   + #### Necesitamos generar listas de palabras en lugar de listas de oraciones.\n",
        "   + #### Eliminar líneas vacías.\n",
        " \n",
        "#### Las cadenas de Python tienen el método [split ()] (https://docs.python.org/2/library/string.html#string.split) que separa una cadena por un separador. En nuestro caso, queremos separar las cadenas por espacio.\n",
        "\n",
        "#### Usa la función `map()` para generar un nuevo RDD como lista de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vxUdcM97R96",
        "outputId": "06b7886e-e55b-4e6f-d65b-d605fb7517a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', 'project', 'gutenbergs', 'the', 'complete']\n",
            "1009881\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split(\" \"))\n",
        "total = shakesPalavrasRDD.count()\n",
        "print (shakesPalavrasRDD.take(5))\n",
        "print (total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUeSCTta7R96"
      },
      "source": [
        "#### Como habrás notado, el uso de la función `map()` genera una lista para cada línea, creando un RDD que contiene una lista de listas.\n",
        "\n",
        "#### Para resolver este problema, Spark tiene una función análoga llamada `flatMap ()` que aplica la transformación `map()`, pero *aplana* el retorno en forma de lista a una lista unidimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m7lREgJ7R97",
        "outputId": "75e88fc3-3171-4ada-b408-b1bb83294b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['project', 'gutenbergs', 'the', 'complete', 'works']\n",
            "959359\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split())\n",
        "total = shakesPalavrasRDD.count()\n",
        "print (shakesPalavrasRDD.take(5))\n",
        "print (total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PcRT7CY7R97",
        "outputId": "9b8481cf-8f35-4088-fbd5-47bf45216e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert total==959359, \"valor incorreto de palavras!\"\n",
        "print (\"OK\")\n",
        "assert shakesPalavrasRDD.take(5)==['project', 'gutenbergs', 'the', 'complete', 'works'],'lista incorreta de palavras'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGlpiwuP7R98"
      },
      "source": [
        "#### Tenga en cuenta que `flatMap` ya ha eliminado las entradas vacías."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hnOX3FJ7R98"
      },
      "source": [
        "#### ** (4f) Número de palabras **\n",
        "#### Ahora que nuestro RDD contiene una lista de palabras, podemos aplicar nuestra función `contaPalavras()`.\n",
        "\n",
        "#### Aplica la función a nuestro RDD y usa la función `takeOrdered` para imprimir las 15 palabras más frecuentes.\n",
        "\n",
        "#### `takeOrdered()` puede tomar un segundo parámetro que le dice a Spark cómo ordenar los elementos. Ex.:\n",
        "\n",
        "#### `takeOrdered (15, key=lambda x: -x)`: orden descendente de valores x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xeM9wUj7R98",
        "outputId": "12d476f1-e69b-4a5d-87b5-2296ac95f7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the: 29996\n",
            "and: 28353\n",
            "i: 21860\n",
            "to: 20885\n",
            "of: 18811\n",
            "a: 15992\n",
            "you: 14439\n",
            "my: 13191\n",
            "in: 12027\n",
            "that: 11782\n",
            "is: 9711\n",
            "not: 9068\n",
            "with: 8521\n",
            "me: 8271\n",
            "for: 8184\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "top15 = contaPalavras(shakesPalavrasRDD).sortBy(lambda x: x[1], ascending=False).take(15) \n",
        "print ('\\n'.join(map(lambda x: f'{x[0]}: {x[1]}', top15)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQlxqWA7R99",
        "outputId": "6ca1296a-eb0a-4640-e831-78b99ebd5082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert top15 == [('the', 29996), ('and', 28353), ('i', 21860), ('to', 20885), ('of', 18811), ('a', 15992), ('you', 14439), ('my', 13191), ('in', 12027), ('that', 11782), ('is', 9711), ('not', 9068), ('with', 8521), ('me', 8271), ('for', 8184)],'valores incorretos!'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auuxZZlf7R99"
      },
      "source": [
        "### **Parte 5: Similaridad entre Objetos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyk7eDat7R9-"
      },
      "source": [
        "### En esta parte del laboratorio aprenderemos a calcular la distancia entre atributos numéricos, categóricos y textuales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjr6YYCC7R9-"
      },
      "source": [
        "#### ** (5a) Vectores en el espacio euclidiano **\n",
        "\n",
        "#### Cuando nuestros objetos están representados en el espacio euclidiano, medimos la similitud entre ellos a través de la *p-Norma* definida por:\n",
        "\n",
        "#### $$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{1/p}$$\n",
        "\n",
        "#### Las normas más utilizadas son $p=1,2,\\infty$ que reducen en distancia absoluta, Euclidiana y distancia máxima:\n",
        "\n",
        "#### $$d(x,y,1) = \\sum_{i=1}^{n}{|x_i - y_i|}$$\n",
        "\n",
        "#### $$d(x,y,2) = (\\sum_{i=1}^{n}{|x_i - y_i|^2})^{1/2}$$\n",
        "\n",
        "#### $$d(x,y,\\infty) = \\max(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udW3YrxS7R9_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Vamos criar uma função pNorm que recebe como parâmetro p e retorna uma função que calcula a pNorma\n",
        "def pNorm(p):\n",
        "    \"\"\"Generates a function to calculate the p-Norm between two points.\n",
        "\n",
        "    Args:\n",
        "        p (int): The integer p.\n",
        "\n",
        "    Returns:\n",
        "        Dist: A function that calculates the p-Norm.\n",
        "    \"\"\"\n",
        "\n",
        "    def Dist(x,y):\n",
        "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
        "    return Dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Jrzyki7R9_"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma RDD com valores numéricos\n",
        "np.random.seed(42)\n",
        "numPointsRDD = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2zksR6B7R-A",
        "outputId": "dcec239e-f906-40b0-ba6d-78a18c1c03bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0 4.709048183663605 3.7511916889753705\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
        "cartPointsRDD = numPointsRDD.cartesian(numPointsRDD)\n",
        "\n",
        "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
        "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
        "cartPointsParesRDD = cartPointsRDD.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "\n",
        "\n",
        "# Aplique um mapa para calcular a Distância Euclidiana entre os pares\n",
        "Euclid = pNorm(2)\n",
        "distRDD = cartPointsParesRDD.map(lambda x: (x[0], Euclid(x[1][0],x[1][1])))\n",
        "\n",
        "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
        "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
        "statRDD = distRDD.map(lambda x: x[1])\n",
        "\n",
        "minv, maxv, meanv = statRDD.min(), statRDD.max(), statRDD.mean()\n",
        "print (minv, maxv, meanv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhSVuyeh7R-A",
        "outputId": "7a70dc34-d119-46b8-d376-42212007a93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert (minv.round(2), maxv.round(2), meanv.round(2))==(0.0, 4.71, 3.75), 'Valores incorretos'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlM73HxB7R-A"
      },
      "source": [
        "#### ** (5b) Valores Categóricos **\n",
        "\n",
        "#### Cuando nuestros objetos están representados por atributos categóricos, no tienen similitud espacial. Para calcular la similitud entre ellos podemos primero transformar nuestro vector de atributos en un vector binario indicando, para cada valor posible de cada atributo, si tiene ese atributo o no.\n",
        "\n",
        "#### Con el vector binario podemos usar la distancia de Hamming definida por:\n",
        "\n",
        "#### $$ H(x,y) = \\sum_{i=1}^{n}{x_i != y_i} $$\n",
        "\n",
        "#### También puedes establecer la distancia de Jaccard como:\n",
        "\n",
        "#### $$ J(x,y) = \\frac{\\sum_{i=1}^{n}{x_i == y_i} }{\\sum_{i=1}^{n}{\\max(x_i, y_i}) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blbobQCl7R-B"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma função para calcular a distância de Hamming\n",
        "def Hamming(x,y):\n",
        "    \"\"\"Calculates the Hamming distance between two binary vectors.\n",
        "\n",
        "    Args:\n",
        "        x, y (np.array): Array of binary integers x and y.\n",
        "\n",
        "    Returns:\n",
        "        H (int): The Hamming distance between x and y.\n",
        "    \"\"\"\n",
        "    return (x!=y).sum()\n",
        "\n",
        "# Vamos criar uma função para calcular a distância de Jaccard\n",
        "def Jaccard(x,y):\n",
        "    \"\"\"Calculates the Jaccard distance between two binary vectors.\n",
        "\n",
        "    Args:\n",
        "        x, y (np.array): Array of binary integers x and y.\n",
        "\n",
        "    Returns:\n",
        "        J (int): The Jaccard distance between x and y.\n",
        "    \"\"\"\n",
        "    return (x==y).sum()/float( np.maximum(x,y).sum() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kuXNwPv7R-B"
      },
      "outputs": [],
      "source": [
        "# Vamos criar uma RDD com valores categóricos\n",
        "catPointsRDD = sc.parallelize(enumerate([['alto', 'caro', 'azul'],\n",
        "                             ['medio', 'caro', 'verde'],\n",
        "                             ['alto', 'barato', 'azul'],\n",
        "                             ['medio', 'caro', 'vermelho'],\n",
        "                             ['baixo', 'barato', 'verde'],\n",
        "                            ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVk_eti27R-C",
        "outputId": "5c060da4-fe14-48a0-ce24-212c67acfa03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alto': 2, 'caro': 0, 'baixo': 4, 'verde': 1, 'azul': 2, 'medio': 3, 'barato': 4, 'vermelho': 3} 8\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Crie um RDD de chaves únicas utilizando flatMap\n",
        "chavesRDD = (catPointsRDD\n",
        "             .flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "             .reduceByKey(lambda x,y: x)\n",
        "             .map(lambda x: x[0])\n",
        "             )\n",
        "\n",
        "chaves = dict((v,k) for k,v in chavesRDD.collect())\n",
        "nchaves = len(chaves)\n",
        "print (chaves, nchaves)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7rGp_N-7R-C",
        "outputId": "5defe6f2-3c16-4625-e9de-6f7c298f0a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert chaves=={'alto': 2, 'caro': 0, 'baixo': 4, 'verde': 1, 'azul': 2, 'medio': 3, 'barato': 4, 'vermelho': 3}, 'valores incorretos!'\n",
        "print (\"OK\")\n",
        "\n",
        "assert nchaves==8, 'número de chaves incorreta'\n",
        "print (\"OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiYZDw497R-C",
        "outputId": "adcc40fc-93de-425b-ed03-190f32f84317"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, array([1., 0., 1., 0., 0., 0., 0., 0.])),\n",
              " (1, array([1., 1., 0., 1., 0., 0., 0., 0.])),\n",
              " (2, array([0., 0., 1., 0., 1., 0., 0., 0.])),\n",
              " (3, array([1., 0., 0., 1., 0., 0., 0., 0.])),\n",
              " (4, array([0., 1., 0., 0., 1., 0., 0., 0.]))]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def CreateNP(atributos,chaves):  \n",
        "    \"\"\"Binarize the categorical vector using a dictionary of keys.\n",
        "\n",
        "    Args:\n",
        "        atributos (list): List of attributes of a given object.\n",
        "        chaves (dict): dictionary with the relation attribute -> index\n",
        "\n",
        "    Returns:\n",
        "        array (np.array): Binary array of attributes.\n",
        "    \"\"\"\n",
        "    \n",
        "    array = np.zeros(len(chaves))\n",
        "    for atr in atributos:\n",
        "        array[ chaves[atr] ] = 1\n",
        "    return array\n",
        "\n",
        "# Converte o RDD para o formato binário, utilizando o dict chaves\n",
        "binRDD = catPointsRDD.map(lambda rec: (rec[0],CreateNP(rec[1], chaves)))\n",
        "binRDD.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK2UUH037R-D",
        "outputId": "ba7203af-828e-4c1a-ac0c-9f4ac9b692bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tMin\tMax\tMean\n",
            "Hamming:\t0.00\t5.00\t2.40\n",
            "Jaccard:\t0.60\t4.00\t1.90\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
        "cartBinRDD = binRDD.cartesian(binRDD)\n",
        "\n",
        "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
        "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
        "cartBinParesRDD = cartBinRDD.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "\n",
        "\n",
        "# Aplique um mapa para calcular a Distância de Hamming e Jaccard entre os pares\n",
        "hamRDD = cartBinParesRDD.map(lambda x: (x[0], Hamming(x[1][0],x[1][1])))\n",
        "jacRDD = cartBinParesRDD.map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "\n",
        "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
        "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
        "statHRDD = hamRDD.map(lambda x: x[1])\n",
        "statJRDD = jacRDD.map(lambda x: x[1])\n",
        "\n",
        "Hmin, Hmax, Hmean = statHRDD.min(), statHRDD.max(), statHRDD.mean()\n",
        "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
        "\n",
        "print (\"\\t\\tMin\\tMax\\tMean\")\n",
        "print (\"Hamming:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(Hmin, Hmax, Hmean ))\n",
        "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odD7uCRR7R-D",
        "outputId": "b8a640be-7f28-4e3d-e9d6-d14a0bed2142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "assert (Hmin.round(2), Hmax.round(2), Hmean.round(2)) == (0.00,5.00,2.40), 'valores incorretos'\n",
        "print (\"OK\")\n",
        "assert (Jmin.round(2), Jmax.round(2), Jmean.round(2)) == (0.60,4.00,1.90), 'valores incorretos'\n",
        "print (\"OK\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Qf8vN7hP7R9q",
        "pGlpiwuP7R98"
      ],
      "name": "laboratorioPySpark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}